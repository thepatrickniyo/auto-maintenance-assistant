{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "head"
      },
      "source": [
        "# Car Maintenance LLM Assistant — LoRA Fine-Tuning (Colab)\n",
        "\n",
        "This notebook fine-tunes **TinyLlama-1.1B-Chat** (or Gemma-2B) on car maintenance Q&A using **LoRA** and **SFTTrainer**.\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime → Change runtime type → T4 GPU\n",
        "2. Upload your `data/training/` folder (train.json) or clone the repo\n",
        "3. Run all cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pip"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload"
      },
      "source": [
        "## 2. Upload training data\n",
        "\n",
        "Upload your `train.json` (and optionally `val.json`) from `data/training/`. Or mount Drive and copy from your project folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "data_upload"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload train.json to data/training/ (e.g. drag-and-drop in file panel).\n"
          ]
        }
      ],
      "source": [
        "# Option A: Upload files manually (run this cell, then use Colab file browser to upload data/training/train.json)\n",
        "import os\n",
        "os.makedirs(\"data/training\", exist_ok=True)\n",
        "\n",
        "# Option B: If you have project in Google Drive, mount and copy:\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# !cp /content/drive/MyDrive/auto-maintenance-assistant/data/training/train.json data/training/\n",
        "\n",
        "if os.path.exists(\"data/training/train.json\"):\n",
        "    print(\"train.json found.\")\n",
        "else:\n",
        "    print(\"Upload train.json to data/training/ (e.g. drag-and-drop in file panel).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 3. Config and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "config_cell"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or \"google/gemma-2b-it\" with appropriate access\n",
        "DATA_DIR = \"data/training\"\n",
        "OUTPUT_DIR = \"car-maintenance-llm\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 2\n",
        "WARMUP_RATIO = 0.03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## 4. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 28 (from /Users/patrickniyo/Documents/auto-maintenance-assistant/data/training)\n"
          ]
        }
      ],
      "source": [
        "# Find train.json: works when run from project root, from notebooks/, or in Colab after upload\n",
        "_candidates = [\n",
        "    Path(\"data/training/train.json\"),                    # project root or Colab\n",
        "    Path.cwd() / \"data\" / \"training\" / \"train.json\",\n",
        "    Path.cwd().parent / \"data\" / \"training\" / \"train.json\",  # run from notebooks/\n",
        "]\n",
        "_train_path = None\n",
        "for _p in _candidates:\n",
        "    if _p.exists():\n",
        "        _train_path = _p\n",
        "        break\n",
        "if _train_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"train.json not found. Run from project root: python scripts/prepare_training_data.py\\n\"\n",
        "        \"Then either run this notebook from project root, or upload data/training/train.json into data/training/.\"\n",
        "    )\n",
        "DATA_DIR = str(_train_path.parent)\n",
        "with open(_train_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "def format_prompt(ex):\n",
        "    inp = ex.get(\"input\", \"\")\n",
        "    if inp:\n",
        "        return f\"### Instruction:\\n{ex['instruction']}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{ex['output']}\"\n",
        "    return f\"### Instruction:\\n{ex['instruction']}\\n\\n### Response:\\n{ex['output']}\"\n",
        "\n",
        "train_texts = [format_prompt(ex) for ex in train_data]\n",
        "dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "print(f\"Train examples: {len(dataset)} (from {DATA_DIR})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## 5. Load model and LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "model_lora"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 201/201 [02:01<00:00,  1.65it/s, Materializing param=model.norm.weight]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 6. Train (SFTTrainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "train_cell"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "Adding EOS to train dataset: 100%|██████████| 28/28 [00:01<00:00, 25.32 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 28/28 [00:00<00:00, 683.75 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 28/28 [00:00<00:00, 8083.74 examples/s]\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
            "/Users/patrickniyo/Documents/auto-maintenance-assistant/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 01:14, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.160878</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/patrickniyo/Documents/auto-maintenance-assistant/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to car-maintenance-llm\n"
          ]
        }
      ],
      "source": [
        "# Use SFTConfig (trl 0.11+): SFT-specific args go here, not in SFTTrainer\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    # SFT-specific (required in SFTConfig for newer trl)\n",
        "    dataset_text_field=\"text\",\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# trl 0.12+: use processing_class instead of tokenizer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## 7. Export / download\n",
        "\n",
        "Download the `car-maintenance-llm` folder (or zip it and download, or copy to Drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zip_download"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: car-maintenance-llm/ (stored 0%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/ (stored 0%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/adapter_model.safetensors (deflated 61%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/rng_state.pth (deflated 26%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/tokenizer_config.json (deflated 46%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/optimizer.pt (deflated 100%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/scheduler.pt (deflated 62%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/tokenizer.json (deflated 85%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/README.md (deflated 65%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/training_args.bin (deflated 53%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/adapter_config.json (deflated 58%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/chat_template.jinja (deflated 60%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/scaler.pt (deflated 64%)\n",
            "  adding: car-maintenance-llm/checkpoint-8/trainer_state.json (deflated 56%)\n",
            "  adding: car-maintenance-llm/adapter_model.safetensors (deflated 61%)\n",
            "  adding: car-maintenance-llm/tokenizer_config.json (deflated 46%)\n",
            "  adding: car-maintenance-llm/tokenizer.json (deflated 85%)\n",
            "  adding: car-maintenance-llm/README.md (deflated 45%)\n",
            "  adding: car-maintenance-llm/training_args.bin (deflated 53%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/ (stored 0%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/adapter_model.safetensors (deflated 61%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/rng_state.pth (deflated 26%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/tokenizer_config.json (deflated 46%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/optimizer.pt (deflated 100%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/scheduler.pt (deflated 61%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/tokenizer.json (deflated 85%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/README.md (deflated 65%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/training_args.bin (deflated 53%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/adapter_config.json (deflated 58%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/chat_template.jinja (deflated 60%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/scaler.pt (deflated 64%)\n",
            "  adding: car-maintenance-llm/checkpoint-4/trainer_state.json (deflated 56%)\n",
            "  adding: car-maintenance-llm/adapter_config.json (deflated 58%)\n",
            "  adding: car-maintenance-llm/chat_template.jinja (deflated 60%)\n",
            "Download the file car-maintenance-llm.zip from the Colab file browser (left panel).\n"
          ]
        }
      ],
      "source": [
        "!zip -r car-maintenance-llm.zip car-maintenance-llm\n",
        "print(\"Download the file car-maintenance-llm.zip from the Colab file browser (left panel).\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "standard",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
